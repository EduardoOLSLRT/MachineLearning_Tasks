//Exerc√≠co 1://

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report, roc_auc_score

# 1. Criar dataset simulado
np.random.seed(42)  # para reprodutibilidade

n_samples = 100

# Features num√©ricas
idade = np.random.randint(25, 66, n_samples)
salario_anual = np.random.randint(30000, 200001, n_samples).astype(float)
anos_empregado = np.random.randint(0, 31, n_samples)
valor_emprestimo = np.random.randint(5000, 100001, n_samples)

# Introduzindo valores ausentes na coluna salario_anual (~10%)
mask_nan = np.random.rand(n_samples) < 0.1
salario_anual[mask_nan] = np.nan

# Features categ√≥ricas
tipos_moradia = ['Aluguel', 'Propria', 'Financiada']
tipo_moradia = np.random.choice(tipos_moradia, n_samples)

# Alvo (target): risco_inadimplencia (0 = Baixo Risco, 1 = Alto Risco)
# Simulei um padr√£o simples: clientes mais jovens, com menor sal√°rio e empr√©stimos maiores t√™m mais risco.
risco_inadimplencia = (
    (idade < 40).astype(int) +
    (salario_anual < 70000).astype(int) +
    (valor_emprestimo > 50000).astype(int)
)
# Risco se soma >= 2
risco_inadimplencia = (risco_inadimplencia >= 2).astype(int)

# Criar DataFrame
df = pd.DataFrame({
    'idade': idade,
    'salario_anual': salario_anual,
    'anos_empregado': anos_empregado,
    'valor_emprestimo': valor_emprestimo,
    'tipo_moradia': tipo_moradia,
    'risco_inadimplencia': risco_inadimplencia
})

# 2. Separar features e target
X = df.drop('risco_inadimplencia', axis=1)
y = df['risco_inadimplencia']

# 3. Dividir em treino e teste (30% teste)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)

# 4. Definir pr√©-processamento
numeric_features = ['idade', 'salario_anual', 'anos_empregado', 'valor_emprestimo']
categorical_features = ['tipo_moradia']

numeric_transformer = Pipeline([
    ('imputer', SimpleImputer(strategy='mean')),   # preencher NaNs com m√©dia
    ('scaler', StandardScaler())
])

categorical_transformer = Pipeline([
    ('onehot', OneHotEncoder(handle_unknown='ignore'))
])

preprocessor = ColumnTransformer([
    ('num', numeric_transformer, numeric_features),
    ('cat', categorical_transformer, categorical_features)
])

# 5. Criar pipeline final com o modelo
model = Pipeline([
    ('preprocessor', preprocessor),
    ('classifier', LogisticRegression(random_state=42))
])

# 6. Treinar o modelo
model.fit(X_train, y_train)

# 7. Fazer previs√µes
y_pred = model.predict(X_test)
y_proba = model.predict_proba(X_test)[:,1]

# 8. Avaliar o modelo
print("Acur√°cia:", accuracy_score(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred))
print("ROC AUC Score:", roc_auc_score(y_test, y_proba))

//Exerc√≠cio 2://

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, r2_score

# 1. Criar dataset simulado
np.random.seed(42)
n_dias = 365  # 1 ano de dados

# Criar coluna de datas
datas = pd.date_range(start='2024-01-01', periods=n_dias, freq='D')

# Temperatura m√©dia simulada (varia ao longo do ano)
temperatura_media = 10 + 15 * np.sin(2 * np.pi * datas.dayofyear / 365) + np.random.normal(0, 2, n_dias)
temperatura_media = np.clip(temperatura_media, 0, 35)  # manter entre 0 e 35 graus

# Dia √∫til: 1 se segunda a sexta, 0 se fim de semana
dia_util = datas.weekday < 5  # segunda=0, domingo=6
dia_util = dia_util.astype(int)

# Consumo de energia simulado (relacionado √† temperatura e se √© dia √∫til)
consumo_energia_kwh = (
    3000 +
    50 * temperatura_media +           # dias quentes consomem mais (ar-condicionado)
    400 * dia_util +                   # dias √∫teis consomem mais
    np.random.normal(0, 200, n_dias)   # ru√≠do
)

# Criar DataFrame
df = pd.DataFrame({
    'data': datas,
    'temperatura_media': temperatura_media,
    'dia_util': dia_util,
    'consumo_energia_kwh': consumo_energia_kwh
})

# 2. Engenharia de atributos a partir da data
df['mes'] = df['data'].dt.month
df['dia_da_semana'] = df['data'].dt.weekday  # 0 = segunda
df['dia_do_ano'] = df['data'].dt.dayofyear

# Remover a coluna de data original
df = df.drop('data', axis=1)

# 3. Separar X e y
X = df.drop('consumo_energia_kwh', axis=1)
y = df['consumo_energia_kwh']

# Dividir em treino e teste (30% teste)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42
)

# 4. Criar pipeline de regress√£o
pipeline = Pipeline([
    ('scaler', StandardScaler()),
    ('regressor', RandomForestRegressor(random_state=42))
])

# 5. Treinar o modelo
pipeline.fit(X_train, y_train)

# 6. Fazer previs√µes
y_pred = pipeline.predict(X_test)

# 7. Avalia√ß√£o do modelo
mae = mean_absolute_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"Erro Absoluto M√©dio (MAE): {mae:.2f} kWh")
print(f"R¬≤ Score: {r2:.3f}")

//Ex√©rcicio 3://

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score, classification_report

# 1. Criando o dataset
dados = pd.DataFrame({
    'texto': [
        "oferta imperd√≠vel clique aqui agora",
        "ganhe dinheiro f√°cil",
        "relat√≥rio de vendas anexo",
        "oi, tudo bem? reuni√£o amanh√£",
        "promo√ß√£o limitada, compre j√°",
        "envie seu curr√≠culo para vaga",
        "parab√©ns, voc√™ ganhou um pr√™mio",
        "atualiza√ß√£o do sistema dispon√≠vel",
        "n√£o perca essa chance √∫nica",
        "reuni√£o de equipe adiada"
    ],
    'categoria': [
        'spam', 'spam', 'ham', 'ham',
        'spam', 'ham', 'spam', 'ham',
        'spam', 'ham'
    ]
})

# 2. Separando features e alvo
X = dados['texto']
y = dados['categoria']

# 3. Dividindo em treino e teste
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42, stratify=y
)

# 4. Criando pipeline com TfidfVectorizer e MultinomialNB
pipeline = Pipeline([
    ('tfidf', TfidfVectorizer()),
    ('clf', MultinomialNB())
])

# 5. Treinando o modelo
pipeline.fit(X_train, y_train)

# 6. Fazendo previs√µes no conjunto de teste
y_pred = pipeline.predict(X_test)

# 7. Avaliando o modelo
print("Acur√°cia:", accuracy_score(y_test, y_pred))
print("\nRelat√≥rio de Classifica√ß√£o:\n", classification_report(y_test, y_pred))

# 8. Coment√°rio sobre alta precis√£o vs alto recall em spam
print("""
üí° Considera√ß√µes:
- Para filtro de spam, √© geralmente mais importante ter um alto recall para a classe 'spam'.
- Isso porque queremos pegar a maior parte dos spams para n√£o deixar passar lixo na caixa de entrada.
- Por√©m, devemos equilibrar para n√£o classificar muitos e-mails leg√≠timos (ham) como spam, o que causa transtorno.
""")


//DESAFIOS//

//Ex√©rcico 4//
# Projeto 4: Classifica√ß√£o de Esp√©cies de Flores √çris (Vers√£o Aprimorada)
# Autor: [Seu Nome]
# Objetivo: Classificar automaticamente a esp√©cie de uma flor √çris com base em medidas de p√©talas e s√©palas.
#           Agora com ajuste de hiperpar√¢metro via GridSearchCV.

# ==========================
# üì¶ 1. Importa√ß√£o das Bibliotecas
# ==========================
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

# ==========================
# üå∏ 2. Carregamento do Dataset
# ==========================
iris = load_iris()
X = iris.data
y = iris.target
classes = iris.target_names

print("üìã Informa√ß√µes do Dataset Iris:")
print(f"Features: {iris.feature_names}")
print(f"Classes: {classes}")
print(f"Tamanho total: {X.shape[0]} amostras\n")

# ==========================
# üîÄ 3. Divis√£o em Treino e Teste
# ==========================
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# ==========================
# üß© 4. Pipeline (StandardScaler + KNN)
# ==========================
pipeline = Pipeline([
    ('scaler', StandardScaler()),
    ('knn', KNeighborsClassifier())
])

# ==========================
# üîß 5. Ajuste Autom√°tico de n_neighbors com GridSearchCV
# ==========================
param_grid = {
    'knn__n_neighbors': [1, 3, 5, 7, 9, 11, 13],
    'knn__weights': ['uniform', 'distance']
}

print("üîç Buscando o melhor n√∫mero de vizinhos (n_neighbors)...")
grid = GridSearchCV(
    pipeline,
    param_grid,
    cv=5,
    scoring='accuracy',
    n_jobs=-1,
    verbose=1
)
grid.fit(X_train, y_train)

# Melhor modelo encontrado
melhor_modelo = grid.best_estimator_
print("\n‚úÖ Melhor combina√ß√£o de par√¢metros encontrada:")
print(grid.best_params_)

# ==========================
# ‚öôÔ∏è 6. Treinamento com os melhores par√¢metros
# ==========================
melhor_modelo.fit(X_train, y_train)

# ==========================
# üîç 7. Avalia√ß√£o do Modelo
# ==========================
y_pred = melhor_modelo.predict(X_test)

accuracy = accuracy_score(y_test, y_pred)
print(f"\nüéØ Acur√°cia do Modelo Otimizado: {accuracy:.3f}\n")

print("üìä Relat√≥rio de Classifica√ß√£o:")
print(classification_report(y_test, y_pred, target_names=classes))

# ==========================
# üìà 8. Visualiza√ß√£o: Matriz de Confus√£o
# ==========================
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(6, 5))
sns.heatmap(cm, annot=True, fmt="d", cmap="Greens",
            xticklabels=classes, yticklabels=classes)
plt.title("üåº Matriz de Confus√£o - Modelo KNN Otimizado")
plt.xlabel("Classe Predita")
plt.ylabel("Classe Real")
plt.show()

# ==========================
# üí¨ 9. Interpreta√ß√£o dos Resultados
# ==========================
print("üí° Interpreta√ß√£o:")
if accuracy >= 0.97:
    print("Excelente! O modelo classifica quase todas as flores corretamente.")
elif accuracy >= 0.9:
    print("Bom desempenho! Pequenos ajustes em n_neighbors ou dados extras podem melhorar ainda mais.")
else:
    print("O desempenho √© razo√°vel. Considere testar outros algoritmos, como SVM ou Random Forest.")

//Exerc√≠cio 5://
